{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNpmaAUihy1W06dxkfiEB1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study3/blob/main/MagnitudeGradientPruning1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZgfryOOBZIr",
        "outputId": "2316e56b-5743-459c-d6b4-3cef46d92ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l4G55YrBb05",
        "outputId": "577d0f7e-9f38-4fb1-bb92-b0a5b58c35aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "SiSDkSNIBggA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드 및 전처리 함수 정의\n",
        "def load_data(base_dir, split, categories):\n",
        "    X = []\n",
        "    y = []\n",
        "    split_dir = os.path.join(base_dir, split)\n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(split_dir, category)\n",
        "        for file in os.listdir(category_dir):\n",
        "            file_path = os.path.join(category_dir, file)\n",
        "            data = pd.read_csv(file_path, header=None).values\n",
        "            data = pd.to_numeric(data.flatten(), errors='coerce').reshape(-1, data.shape[1])\n",
        "            data = np.nan_to_num(data).astype('float32')  # NaN 값을 0으로 대체하고, float32로 변환\n",
        "            X.append(data)\n",
        "            y.append(category)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 기본 경로 및 카테고리 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_motor10'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "\n",
        "# 데이터 로드\n",
        "X_train, y_train = load_data(base_dir, 'train', categories)\n",
        "X_val, y_val = load_data(base_dir, 'validation', categories)\n",
        "X_test, y_test = load_data(base_dir, 'test', categories)\n",
        "\n",
        "# 데이터 차원 변경 (CNN 입력 형식에 맞게)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# 원-핫 인코딩\n",
        "y_train_categorical = tf.keras.utils.to_categorical(y_train_encoded)\n",
        "y_val_categorical = tf.keras.utils.to_categorical(y_val_encoded)\n",
        "y_test_categorical = tf.keras.utils.to_categorical(y_test_encoded)"
      ],
      "metadata": {
        "id": "VTq80HW6Bb3n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "model = models.Sequential([\n",
        "    layers.Conv1D(filters=64, kernel_size=16, strides=16, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    layers.MaxPooling1D(pool_size=2, strides=2),\n",
        "    layers.Conv1D(filters=32, kernel_size=3, strides=1, activation='relu'),\n",
        "    layers.Conv1D(filters=64, kernel_size=5, strides=1, activation='relu'),\n",
        "    layers.Conv1D(filters=128, kernel_size=5, strides=1, activation='relu'),\n",
        "    layers.MaxPooling1D(pool_size=2, strides=2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(units=5000, activation='relu'),\n",
        "    layers.Dense(units=1000, activation='relu'),\n",
        "    layers.Dense(len(categories), activation='softmax')\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "S8L68tBLBb7_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련\n",
        "history = model.fit(X_train, y_train_categorical, epochs=10, validation_data=(X_val, y_val_categorical), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQOlfDGFBb_B",
        "outputId": "d0e55506-c45d-4d76-d8c5-e5754fe07a20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "394/394 [==============================] - 236s 593ms/step - loss: 0.9495 - accuracy: 0.6230 - val_loss: 0.3139 - val_accuracy: 0.8996\n",
            "Epoch 2/10\n",
            "394/394 [==============================] - 232s 588ms/step - loss: 0.3497 - accuracy: 0.8490 - val_loss: 0.1133 - val_accuracy: 0.9530\n",
            "Epoch 3/10\n",
            "394/394 [==============================] - 233s 591ms/step - loss: 0.0633 - accuracy: 0.9789 - val_loss: 0.1192 - val_accuracy: 0.9489\n",
            "Epoch 4/10\n",
            "394/394 [==============================] - 232s 590ms/step - loss: 0.0377 - accuracy: 0.9863 - val_loss: 0.0055 - val_accuracy: 0.9993\n",
            "Epoch 5/10\n",
            "394/394 [==============================] - 233s 591ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0024 - val_accuracy: 0.9996\n",
            "Epoch 6/10\n",
            "394/394 [==============================] - 231s 586ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 1.9125e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "394/394 [==============================] - 230s 584ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0200 - val_accuracy: 0.9919\n",
            "Epoch 8/10\n",
            "394/394 [==============================] - 229s 581ms/step - loss: 9.3318e-04 - accuracy: 0.9996 - val_loss: 8.5935e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "394/394 [==============================] - 228s 579ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 7.6340e-06 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "394/394 [==============================] - 228s 579ms/step - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.0010 - val_accuracy: 0.9996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프루닝 적용 코드\n",
        "def magnitude_based_pruning(layer, sparsity):\n",
        "    \"\"\"매그니튜드 기반 프루닝\"\"\"\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        weights, biases = layer.get_weights()\n",
        "        threshold = np.percentile(np.abs(weights), sparsity * 100)\n",
        "        pruned_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
        "        layer.set_weights([pruned_weights, biases])\n",
        "\n",
        "def gradient_sensitivity_pruning(layer, gradients, threshold_ratio):\n",
        "    \"\"\"변화도 기반 프루닝\"\"\"\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        weights, biases = layer.get_weights()\n",
        "        gradient_sensitivity = np.abs(gradients)\n",
        "        threshold = np.percentile(gradient_sensitivity, threshold_ratio * 100)\n",
        "        pruned_weights = np.where(gradient_sensitivity < threshold, 0, weights)\n",
        "        layer.set_weights([pruned_weights, biases])\n",
        "\n",
        "def combined_pruning(model, X_val, y_val, magnitude_sparsity=0.5, gradient_threshold=0.2):\n",
        "    # 기울기 정보는 학습 과정에서 구해야 함\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(X_val, training=True)\n",
        "        loss_value = tf.keras.losses.categorical_crossentropy(y_val, logits)\n",
        "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "    gradient_index = 0\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel'):\n",
        "            # 1단계: 매그니튜드 기반 프루닝\n",
        "            magnitude_based_pruning(layer, magnitude_sparsity)\n",
        "            # 2단계: 변화도 기반 프루닝\n",
        "            gradient_shape = layer.get_weights()[0].shape\n",
        "            relevant_gradients = gradients[gradient_index].numpy().reshape(gradient_shape)\n",
        "            gradient_sensitivity_pruning(layer, relevant_gradients, gradient_threshold)\n",
        "            gradient_index += 2  # 각 레이어에 대해 커널과 바이어스가 있으므로 2씩 증가\n",
        "\n",
        "            # 바이어스가 없는 경우에 대한 처리\n",
        "            if len(layer.get_weights()) == 1:\n",
        "                gradient_index -= 1  # 인덱스 보정\n",
        "\n",
        "# 조합된 프루닝 수행\n",
        "combined_pruning(model, X_val, y_val_categorical, magnitude_sparsity=0.9, gradient_threshold=0.5)"
      ],
      "metadata": {
        "id": "BT67jsHFBcBf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스트립 프루닝 적용\n",
        "model_stripped = tfmot.sparsity.keras.strip_pruning(model)"
      ],
      "metadata": {
        "id": "qoUVD-mVBcEH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프루닝 후 재학습 (파인 튜닝)\n",
        "model_stripped.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_finetune = model_stripped.fit(X_train, y_train_categorical, epochs=5, validation_data=(X_val, y_val_categorical), batch_size=32)"
      ],
      "metadata": {
        "id": "1qGzKY3VBcGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db450d38-8947-40f6-a5db-df928bd6c1b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "394/394 [==============================] - 240s 604ms/step - loss: 0.2330 - accuracy: 0.9198 - val_loss: 0.0532 - val_accuracy: 0.9770\n",
            "Epoch 2/5\n",
            "394/394 [==============================] - 236s 599ms/step - loss: 0.0323 - accuracy: 0.9891 - val_loss: 0.0021 - val_accuracy: 0.9996\n",
            "Epoch 3/5\n",
            "394/394 [==============================] - 240s 608ms/step - loss: 0.0211 - accuracy: 0.9921 - val_loss: 0.0043 - val_accuracy: 0.9981\n",
            "Epoch 4/5\n",
            "394/394 [==============================] - 238s 605ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 8.1616e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "394/394 [==============================] - 239s 607ms/step - loss: 2.2803e-04 - accuracy: 1.0000 - val_loss: 4.6823e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "test_loss, test_accuracy = model_stripped.evaluate(X_test, y_test_categorical)\n",
        "print(f'Test accuracy after combined pruning, stripping, and fine-tuning: {test_accuracy}')"
      ],
      "metadata": {
        "id": "2UFTwXYQBcJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9454e5-2734-4636-c57c-d1ed4ae2d8ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - 4s 43ms/step - loss: 9.8759e-04 - accuracy: 0.9996\n",
            "Test accuracy after combined pruning, stripping, and fine-tuning: 0.9996296167373657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프루닝 스트립 (추론 시 프루닝 마스크 제거)\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_stripped)\n",
        "\n",
        "# 프루닝된 모델 저장\n",
        "model_for_export.save('pruned_model.h5')\n",
        "\n",
        "# 프루닝된 모델 사이즈 측정\n",
        "pruned_model_size = os.path.getsize('pruned_model.h5') / (1024 * 1024)\n",
        "print(f\"Pruned Model Size: {pruned_model_size:.2f} MB\")"
      ],
      "metadata": {
        "id": "cwfChyveGWfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77dcfa3-8892-43de-822e-7d5aed741c20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned Model Size: 463.71 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스트립 프루닝 후 모델 저장\n",
        "model_stripped.save('pruned_model_stripped.h5')"
      ],
      "metadata": {
        "id": "V9ew_VX3BcLh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_non_zero_weights(model):\n",
        "    total_weights = 0\n",
        "    non_zero_weights = 0\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel'):\n",
        "            weights, biases = layer.get_weights()\n",
        "            total_weights += np.prod(weights.shape) + np.prod(biases.shape)\n",
        "            non_zero_weights += np.count_nonzero(weights) + np.count_nonzero(biases)\n",
        "    return non_zero_weights, total_weights\n",
        "\n",
        "non_zero_weights_after, total_weights_after = calculate_non_zero_weights(model_stripped)\n",
        "print(f\"After pruning: Non-zero weights = {non_zero_weights_after}, Total weights = {total_weights_after}\")\n",
        "print(f\"Final non-zero weight ratio: {non_zero_weights_after / total_weights_after:.4f}\")"
      ],
      "metadata": {
        "id": "YHvWwQp3BcQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47da0e4-e107-41aa-9307-f6d74f432da7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After pruning: Non-zero weights = 37902935, Total weights = 121549684\n",
            "Final non-zero weight ratio: 0.3118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6KT4C_qbBcTI"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}