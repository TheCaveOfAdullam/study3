{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPQRVhINyFFk/R1WaqVL9nk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study3/blob/main/1017Test1wow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmMfRTxZaly",
        "outputId": "13e37364-5049-49cc-bc28-dbbca59dd4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_pruning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE-1giakZ1eO",
        "outputId": "4e89479d-acfe-4586-c137-276b99b4dd5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_pruning in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (2.4.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch_pruning) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch_pruning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch_pruning as tp"
      ],
      "metadata": {
        "id": "MzV9NNC7Z1g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_motor10'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "class VibrationDataset(Dataset):\n",
        "    def __init__(self, base_dir, split, categories, transform=None):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.transform = transform\n",
        "        split_dir = os.path.join(base_dir, split)\n",
        "        for category in categories:\n",
        "            category_dir = os.path.join(split_dir, category)\n",
        "            files = os.listdir(category_dir)\n",
        "            for file in files:\n",
        "                file_path = os.path.join(category_dir, file)\n",
        "                data = pd.read_csv(file_path, header=None).values\n",
        "                data = pd.to_numeric(data.flatten(), errors='coerce').reshape(-1, data.shape[1])\n",
        "                data = np.nan_to_num(data).astype('float32')  # NaN 값을 0으로 대체하고, float32로 변환\n",
        "                self.X.append(data)\n",
        "                self.y.append(category)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        X = X.reshape(-1)  # 마지막 차원을 시퀀스에 병합하여 [sequence_length]로 변환\n",
        "        return torch.tensor(X, dtype=torch.float32), y\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_dataset = VibrationDataset(base_dir, 'train', categories)\n",
        "val_dataset = VibrationDataset(base_dir, 'validation', categories)\n",
        "test_dataset = VibrationDataset(base_dir, 'test', categories)\n",
        "\n",
        "# 레이블 인코딩 및 원-핫 인코딩\n",
        "y_train_encoded = label_encoder.fit_transform([y for _, y in train_dataset])\n",
        "y_val_encoded = label_encoder.transform([y for _, y in val_dataset])\n",
        "y_test_encoded = label_encoder.transform([y for _, y in test_dataset])\n",
        "\n",
        "# 데이터셋에 레이블 추가\n",
        "train_dataset.y = y_train_encoded\n",
        "val_dataset.y = y_val_encoded\n",
        "test_dataset.y = y_test_encoded\n",
        "\n",
        "# 데이터 로더\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "BwAOPBOfZ1jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=16, stride=16)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=1)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        conv1_output_size = (24002 - 16) // 16 + 1\n",
        "        pool1_output_size = conv1_output_size // 2\n",
        "        conv2_output_size = (pool1_output_size - 3) // 1 + 1\n",
        "        conv3_output_size = (conv2_output_size - 5) // 1 + 1\n",
        "        conv4_output_size = (conv3_output_size - 5) // 1 + 1\n",
        "        pool2_output_size = conv4_output_size // 2\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * pool2_output_size, 5000)\n",
        "        self.fc2 = nn.Linear(5000, 1000)\n",
        "        self.fc3 = nn.Linear(1000, len(categories))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RvDYGjfRZ1li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매그니튜드 기반 프루닝 함수\n",
        "def prune_by_magnitude(model, threshold):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=threshold)\n",
        "            prune.remove(module, 'weight')\n",
        "    print(f'Magnitude-based pruning with threshold: {threshold} applied.')\n",
        "\n",
        "# 1차 테일러 전개 기반 프루닝 함수\n",
        "def prune_by_taylor(model, threshold):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "            if module.weight.grad is None:\n",
        "                raise ValueError(f\"Gradients not found for {name}. Make sure to run backward pass before pruning.\")\n",
        "\n",
        "            # 중요도 계산 (Taylor 기반)\n",
        "            importance = torch.abs(module.weight * module.weight.grad)\n",
        "            mask = importance > threshold\n",
        "            with torch.no_grad():\n",
        "                module.weight[~mask] = 0\n",
        "    print(f'Taylor expansion-based pruning with threshold: {threshold} applied.')\n",
        "\n",
        "# 필터 감지 함수 (0 비율 기반)\n",
        "def detect_filters_to_prune(model, threshold=0.7):\n",
        "    filters_to_prune = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d):\n",
        "            weight_data = module.weight.detach().cpu().numpy()\n",
        "            filter_zero_percentage = np.mean(weight_data == 0, axis=(1, 2))\n",
        "            prune_indices = np.where(filter_zero_percentage >= threshold)[0]\n",
        "\n",
        "            if len(prune_indices) > 0 and len(prune_indices) < module.weight.shape[0]:\n",
        "                filters_to_prune.append((module, prune_indices))\n",
        "                print(f\"{name}: {len(prune_indices)} filters detected for pruning.\")\n",
        "            else:\n",
        "                print(f\"Skipping pruning for {name} as it would remove all filters.\")\n",
        "    return filters_to_prune\n",
        "\n",
        "# 구조적 프루닝 및 채널 불일치 해결 함수\n",
        "def apply_structural_pruning_with_torchprune(model, filters_to_prune, example_inputs=None):\n",
        "    if example_inputs is None:\n",
        "        example_inputs = torch.randn(1, 1, 24002).to(next(model.parameters()).device)\n",
        "\n",
        "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
        "\n",
        "    total_pruned = 0\n",
        "    for module, prune_indices in filters_to_prune:\n",
        "        pruning_group = DG.get_pruning_group(module, tp.prune_conv_out_channels, idxs=prune_indices)\n",
        "\n",
        "        if pruning_group is not None:\n",
        "            pruning_group.prune()\n",
        "            total_pruned += len(prune_indices)\n",
        "            print(f\"Pruned {len(prune_indices)} filters from module {module}.\")\n",
        "        else:\n",
        "            print(f\"Skipping module {module} as no pruning group was generated.\")\n",
        "\n",
        "    print(f\"Structural pruning applied. {total_pruned} filters pruned in total.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "bPhOUUW2Z1oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수\n",
        "# 모델 학습 함수\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, max_norm=1.0):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.unsqueeze(1).to(device), labels.to(device)\n",
        "            optimizer.zero_grad()  # 옵티마이저 초기화\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()  # 역전파 수행\n",
        "\n",
        "            # 그래디언트 클리핑 적용\n",
        "            utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "            optimizer.step()  # 옵티마이저 업데이트\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/total:.4f}, Accuracy: {100 * correct/total:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    return model  # 학습된 모델 반환\n",
        "\n",
        "# 모델 평가\n",
        "def evaluate_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.unsqueeze(1).to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return val_loss / total, 100 * correct / total"
      ],
      "metadata": {
        "id": "W8ez9RlUZ1qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 과정 함수\n",
        "def prune_and_retrain(model, train_loader, val_loader, criterion, device, optimizer_params, threshold_mag=0.1, threshold_taylor=0.01, prune_threshold=0.7):\n",
        "    # 1. 매그니튜드 기반 프루닝\n",
        "    print(\"Step 1: Magnitude-based pruning\")\n",
        "    prune_by_magnitude(model, threshold_mag)\n",
        "\n",
        "    # 옵티마이저 및 재학습\n",
        "    optimizer = optim.Adam(model.parameters(), **optimizer_params)\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "    # 경사도 계산을 위해 모델을 한 번 더 학습 모드로 돌리고 백워드 패스 진행\n",
        "    model.train()  # 이제 모델이 정상적으로 학습 모드로 진입 가능\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.unsqueeze(1).to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # 경사도 계산\n",
        "        break  # 경사도만 계산하면 되므로 한 배치만 실행\n",
        "\n",
        "    # 2. 1차 테일러 전개 기반 프루닝\n",
        "    print(\"Step 2: Taylor expansion-based pruning\")\n",
        "    prune_by_taylor(model, threshold_taylor)\n",
        "\n",
        "    # 옵티마이저 및 재학습\n",
        "    optimizer = optim.Adam(model.parameters(), **optimizer_params)\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "    # 3. 0 비율이 70% 이상인 필터 감지 및 구조적 프루닝\n",
        "    print(\"Step 3: Structural pruning based on zero ratio\")\n",
        "    filters_to_prune = detect_filters_to_prune(model, threshold=prune_threshold)\n",
        "\n",
        "    # 구조적 프루닝 및 재학습\n",
        "    model = apply_structural_pruning_with_torchprune(model, filters_to_prune)\n",
        "    optimizer = optim.Adam(model.parameters(), **optimizer_params)\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0EkFBfS1Z1sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화 및 파라미터 설정\n",
        "model = CNNModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 모델 학습에 사용할 기본 파라미터\n",
        "optimizer_params = {\n",
        "    'lr': 0.0001,\n",
        "    'weight_decay': 1e-5\n",
        "}"
      ],
      "metadata": {
        "id": "265eYEFTZ1u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프루닝 및 재학습 실행\n",
        "model = prune_and_retrain(model, train_loader, val_loader, criterion, device, optimizer_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knJf0KCSZ1xF",
        "outputId": "25df1e50-5d28-4f9c-dcea-0bf861fe754e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Magnitude-based pruning\n",
            "Magnitude-based pruning with threshold: 0.1 applied.\n",
            "Epoch [1/5], Loss: 0.0391, Accuracy: 49.85%, Val Loss: 0.0393, Val Accuracy: 50.00%\n",
            "Epoch [2/5], Loss: 0.0389, Accuracy: 50.00%, Val Loss: 0.0388, Val Accuracy: 50.00%\n",
            "Epoch [3/5], Loss: 0.0228, Accuracy: 66.98%, Val Loss: 0.0143, Val Accuracy: 83.33%\n",
            "Epoch [4/5], Loss: 0.0128, Accuracy: 83.08%, Val Loss: 0.0240, Val Accuracy: 41.81%\n",
            "Epoch [5/5], Loss: 0.0096, Accuracy: 86.37%, Val Loss: 0.0176, Val Accuracy: 55.63%\n",
            "Step 2: Taylor expansion-based pruning\n",
            "Taylor expansion-based pruning with threshold: 0.01 applied.\n",
            "Epoch [1/5], Loss: 0.0174, Accuracy: 75.71%, Val Loss: 0.0060, Val Accuracy: 99.48%\n",
            "Epoch [2/5], Loss: 0.0100, Accuracy: 86.79%, Val Loss: 0.0055, Val Accuracy: 98.78%\n",
            "Epoch [3/5], Loss: 0.0088, Accuracy: 89.48%, Val Loss: 0.0025, Val Accuracy: 99.85%\n",
            "Epoch [4/5], Loss: 0.0119, Accuracy: 84.15%, Val Loss: 0.0041, Val Accuracy: 95.15%\n",
            "Epoch [5/5], Loss: 0.0084, Accuracy: 89.21%, Val Loss: 0.0048, Val Accuracy: 89.78%\n",
            "Step 3: Structural pruning based on zero ratio\n",
            "conv1: 9 filters detected for pruning.\n",
            "Skipping pruning for conv2 as it would remove all filters.\n",
            "conv3: 4 filters detected for pruning.\n",
            "conv4: 9 filters detected for pruning.\n",
            "Pruned 9 filters from module Conv1d(1, 55, kernel_size=(16,), stride=(16,)).\n",
            "Pruned 4 filters from module Conv1d(32, 60, kernel_size=(5,), stride=(1,)).\n",
            "Pruned 9 filters from module Conv1d(60, 119, kernel_size=(5,), stride=(1,)).\n",
            "Structural pruning applied. 22 filters pruned in total.\n",
            "Epoch [1/5], Loss: 0.0132, Accuracy: 86.24%, Val Loss: 0.0056, Val Accuracy: 89.85%\n",
            "Epoch [2/5], Loss: 0.0117, Accuracy: 87.51%, Val Loss: 0.0002, Val Accuracy: 100.00%\n",
            "Epoch [3/5], Loss: 0.0102, Accuracy: 89.54%, Val Loss: 0.0003, Val Accuracy: 99.89%\n",
            "Epoch [4/5], Loss: 0.0012, Accuracy: 98.87%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
            "Epoch [5/5], Loss: 0.0039, Accuracy: 96.48%, Val Loss: 0.0000, Val Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}