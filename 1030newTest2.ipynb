{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOtP1b7NKHoT4FiHAktSMq7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study3/blob/main/1030newTest2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa05AGaYVKn7",
        "outputId": "94a9fdb2-b8c2-4ef9-e53e-6a3a7d798ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_pruning\n",
        "!pip install ptflops\n",
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh2b8oXpVeem",
        "outputId": "eefd2a3a-09be-4b24-9045-aaf59c270582"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_pruning in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (2.5.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch_pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch_pruning) (3.0.2)\n",
            "Requirement already satisfied: ptflops in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.5)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (2.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch_pruning as tp\n",
        "import time\n",
        "from ptflops import get_model_complexity_info\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "from collections import defaultdict  # defaultdict 임포트"
      ],
      "metadata": {
        "id": "8YgltxsUVehu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_motor10'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "class VibrationDataset(Dataset):\n",
        "    def __init__(self, base_dir, split, categories, label_encoder, transform=None):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.transform = transform\n",
        "        self.file_count = defaultdict(int)  # 카테고리별 파일 개수를 저장할 딕셔너리\n",
        "\n",
        "        split_dir = os.path.join(base_dir, split)\n",
        "        for category in categories:\n",
        "            category_dir = os.path.join(split_dir, category)\n",
        "            files = os.listdir(category_dir)\n",
        "            self.file_count[category] = len(files)  # 카테고리별 파일 수 저장\n",
        "\n",
        "            for file in files:\n",
        "                file_path = os.path.join(category_dir, file)\n",
        "                data = pd.read_csv(file_path, header=None, skiprows=1)  # 첫 행 건너뜀\n",
        "                data = data.apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "                self.X.append(data)  # 시간과 주파수를 모두 포함\n",
        "                self.y.append(label_encoder.transform([category])[0])\n",
        "\n",
        "        # 카테고리별 파일 개수 출력\n",
        "        print(f\"File count for '{split}' split:\")\n",
        "        for category, count in self.file_count.items():\n",
        "            print(f\"  {category}: {count} files\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx].T  # 2D 입력 (채널, 길이)로 변경\n",
        "        y = self.y[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(categories)\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_dataset = VibrationDataset(base_dir, 'train', categories, label_encoder)\n",
        "val_dataset = VibrationDataset(base_dir, 'validation', categories, label_encoder)\n",
        "test_dataset = VibrationDataset(base_dir, 'test', categories, label_encoder)\n",
        "\n",
        "# 데이터 로더\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgCL2NTJVekO",
        "outputId": "4254de4b-958c-4dde-c632-a9169606e391"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File count for 'train' split:\n",
            "  normal: 6300 files\n",
            "  fault_BB: 2100 files\n",
            "  fault_RI: 2100 files\n",
            "  fault_SM: 2100 files\n",
            "File count for 'validation' split:\n",
            "  normal: 1350 files\n",
            "  fault_BB: 450 files\n",
            "  fault_RI: 450 files\n",
            "  fault_SM: 450 files\n",
            "File count for 'test' split:\n",
            "  normal: 1350 files\n",
            "  fault_BB: 450 files\n",
            "  fault_RI: 450 files\n",
            "  fault_SM: 450 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 데이터의 크기를 이용해 input_length 결정\n",
        "first_sample, _ = train_dataset[0]\n",
        "input_length = first_sample.shape[1]  # X는 (채널, 길이) 형태이므로 길이는 첫 번째 데이터의 두 번째 차원\n",
        "print(f\"Input length for one sample: {input_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clHih_jgVem3",
        "outputId": "4ef88009-af8e-4408-a587-2227b94af4cc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input length for one sample: 12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 정의\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_length):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=16, stride=16)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=1)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # 출력 크기 계산\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.zeros(1, 2, input_length)  # 2는 입력 채널 수\n",
        "            sample_output = self.forward_conv_layers(sample_input)\n",
        "            conv_output_size = sample_output.size(1) * sample_output.size(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(conv_output_size, 5000)\n",
        "        self.fc2 = nn.Linear(5000, 1000)\n",
        "        self.fc3 = nn.Linear(1000, len(categories))\n",
        "\n",
        "    def forward_conv_layers(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.pool2(torch.relu(self.conv4(x)))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flattening\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EWwhvAGUVepm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, max_norm=1.0):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()  # 옵티마이저 초기화\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()  # 역전파 수행\n",
        "\n",
        "            # 그래디언트 클리핑 적용\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "\n",
        "            optimizer.step()  # 옵티마이저 업데이트\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/total:.4f}, Accuracy: {100 * correct/total:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return val_loss / total, 100 * correct / total"
      ],
      "metadata": {
        "id": "omV-mRJJVer-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 크기 계산\n",
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    model_size = (param_size + buffer_size) / 1024**2  # Convert to MB\n",
        "    return model_size\n",
        "\n",
        "# 추론 시간 및 메모리 사용량 계산\n",
        "def calculate_inference_time_and_memory(model, input_data, device):\n",
        "    model.eval()\n",
        "    input_data = input_data.to(device)\n",
        "\n",
        "    # 추론 시간 측정\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_data)\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    # 메모리 사용량 측정\n",
        "    if device.type == 'cuda':\n",
        "        memory_usage = torch.cuda.memory_allocated(device) / 1024**2  # Convert to MB\n",
        "    else:\n",
        "        memory_usage = 0  # CPU 메모리 사용량은 별도 라이브러리가 필요합니다.\n",
        "\n",
        "    return inference_time, memory_usage"
      ],
      "metadata": {
        "id": "k7F1K51eVfOP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1차 테일러 전개 기반 비구조적 프루닝 (마스크 없이)\n",
        "def prune_by_taylor(model, threshold=0.01):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "            if module.weight.grad is None:\n",
        "                raise ValueError(f\"Gradients not found for {name}. Run backward pass before pruning.\")\n",
        "\n",
        "            # 가중치 중요도를 계산하고 threshold 이하의 값들을 0으로 설정\n",
        "            importance = torch.abs(module.weight * module.weight.grad)\n",
        "            with torch.no_grad():\n",
        "                module.weight[importance < threshold] = 0\n",
        "    print(f\"Taylor expansion-based pruning with threshold: {threshold} applied.\")\n",
        "\n",
        "# 0 비율 기반 필터 감지 및 구조적 프루닝 적용 함수\n",
        "def detect_and_apply_structural_pruning_with_zero_ratio(model, prune_threshold=0.7, example_inputs=None):\n",
        "    if example_inputs is None:\n",
        "        example_inputs = torch.randn(1, 2, 12000).to(next(model.parameters()).device)\n",
        "\n",
        "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
        "    total_pruned = 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "            # **출력 레이어는 프루닝 대상에서 제외**\n",
        "            if name == 'fc3':\n",
        "                print(f\"Skipping pruning for {name} (output layer).\")\n",
        "                continue\n",
        "\n",
        "            # 필터/뉴런의 0 비율 계산 및 프루닝 대상 선정\n",
        "            weight_data = module.weight.detach().cpu().numpy()\n",
        "            if isinstance(module, nn.Conv1d):\n",
        "                filter_zero_percentage = np.mean(weight_data == 0, axis=(1, 2))\n",
        "            else:\n",
        "                filter_zero_percentage = np.mean(weight_data == 0, axis=1)\n",
        "            prune_indices = np.where(filter_zero_percentage >= prune_threshold)[0]\n",
        "\n",
        "            # 프루닝 대상 필터/뉴런 제거\n",
        "            if len(prune_indices) > 0 and len(prune_indices) < module.weight.shape[0]:\n",
        "                pruning_group = None\n",
        "                if isinstance(module, nn.Conv1d):\n",
        "                    pruning_group = DG.get_pruning_group(module, tp.prune_conv_out_channels, idxs=prune_indices)\n",
        "                elif isinstance(module, nn.Linear):\n",
        "                    pruning_group = DG.get_pruning_group(module, tp.prune_linear_out_channels, idxs=prune_indices)\n",
        "\n",
        "                if pruning_group is not None:\n",
        "                    pruning_group.prune()\n",
        "                    total_pruned += len(prune_indices)\n",
        "                    print(f\"Pruned {len(prune_indices)} filters/neuron(s) from {name}.\")\n",
        "            else:\n",
        "                print(f\"Skipping pruning for {name} as it would remove all filters/neuron(s).\")\n",
        "\n",
        "    print(f\"Structural pruning based on zero ratio applied. {total_pruned} filters/neuron(s) pruned in total.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "DHH52heEVfQ1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프루닝 및 재학습 과정\n",
        "def prune_and_retrain(model, train_loader, val_loader, test_loader, criterion, device, optimizer_params, threshold_taylor=0.01, prune_threshold=0.7):\n",
        "    # 초기 옵티마이저 설정\n",
        "    optimizer = optim.Adam(model.parameters(), **optimizer_params)\n",
        "\n",
        "    # 프루닝 전 초기 훈련\n",
        "    print(\"Initial training before pruning\")\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=7)\n",
        "\n",
        "    # Step 1: Taylor 전개 기반 비구조적 프루닝\n",
        "    print(\"Step 1: Taylor expansion-based pruning\")\n",
        "    # 프루닝을 위해 한 번의 forward 및 backward 패스를 수행하여 그래디언트 계산\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        break  # 한 배치만 필요함\n",
        "\n",
        "    # 마스크 없이 Taylor 기반 프루닝 적용\n",
        "    prune_by_taylor(model, threshold_taylor)\n",
        "\n",
        "    # Step 2: 0 비율 기반 구조적 프루닝\n",
        "    print(\"Step 2: Structural pruning based on zero ratio\")\n",
        "    model = detect_and_apply_structural_pruning_with_zero_ratio(model, prune_threshold=prune_threshold)\n",
        "\n",
        "    # 프루닝 후 재학습을 위한 옵티마이저 재설정\n",
        "    optimizer = optim.Adam(model.parameters(), **optimizer_params)\n",
        "    print(\"Retraining after pruning\")\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=8)\n",
        "\n",
        "    # 테스트 셋에 대한 최종 평가\n",
        "    print(\"Final evaluation on the test set...\")\n",
        "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "    print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a7EtSWkCVfTu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 설정 및 프루닝 실행\n",
        "model = CNNModel(input_length=input_length).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_params = {'lr': 0.0001, 'weight_decay': 1e-5}"
      ],
      "metadata": {
        "id": "BUfizWq7VfWe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 비제로 가중치 계산 함수\n",
        "def count_nonzero_weights(model):\n",
        "    nonzero_count = 0\n",
        "    total_count = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            nonzero_count += torch.sum(param != 0).item()  # 0이 아닌 가중치 수 계산\n",
        "            total_count += param.numel()  # 전체 가중치 수 계산\n",
        "    return nonzero_count, total_count\n",
        "\n",
        "# 비제로 가중치 수 계산\n",
        "nonzero_weights, total_weights = count_nonzero_weights(model)\n",
        "print(f\"Number of non-zero weights: {nonzero_weights}\")\n",
        "print(f\"Total number of weights: {total_weights}\")\n",
        "print(f\"Percentage of non-zero weights: {100 * nonzero_weights / total_weights:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP4jTtGiVfZO",
        "outputId": "948072ca-d522-4b1b-9e72-243a8ac243e2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of non-zero weights: 121549680\n",
            "Total number of weights: 121549684\n",
            "Percentage of non-zero weights: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 크기\n",
        "model_size = get_model_size(model)\n",
        "print(f\"Model Size: {model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvG985MlVfbv",
        "outputId": "733ba641-6a1c-4cf0-ae1d-f1b4cd093698"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 463.68 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = prune_and_retrain(model, train_loader, val_loader, test_loader, criterion, device, optimizer_params, threshold_taylor=0.00001, prune_threshold=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTXiSA4_VfeX",
        "outputId": "49c8461f-94fc-49d4-9402-73132f034626"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial training before pruning\n",
            "Epoch [1/7], Loss: 0.0391, Accuracy: 49.87%, Val Loss: 0.0390, Val Accuracy: 50.00%\n",
            "Epoch [2/7], Loss: 0.0232, Accuracy: 66.40%, Val Loss: 0.0112, Val Accuracy: 89.11%\n",
            "Epoch [3/7], Loss: 0.0065, Accuracy: 91.90%, Val Loss: 0.0029, Val Accuracy: 98.74%\n",
            "Epoch [4/7], Loss: 0.0034, Accuracy: 95.61%, Val Loss: 0.0013, Val Accuracy: 98.93%\n",
            "Epoch [5/7], Loss: 0.0012, Accuracy: 98.49%, Val Loss: 0.0002, Val Accuracy: 100.00%\n",
            "Epoch [6/7], Loss: 0.0003, Accuracy: 99.75%, Val Loss: 0.0001, Val Accuracy: 100.00%\n",
            "Epoch [7/7], Loss: 0.0002, Accuracy: 99.86%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
            "Step 1: Taylor expansion-based pruning\n",
            "Taylor expansion-based pruning with threshold: 1e-05 applied.\n",
            "Step 2: Structural pruning based on zero ratio\n",
            "Pruned 13 filters/neuron(s) from conv1.\n",
            "Pruned 11 filters/neuron(s) from conv2.\n",
            "Pruned 22 filters/neuron(s) from conv3.\n",
            "Pruned 54 filters/neuron(s) from conv4.\n",
            "Skipping pruning for fc1 as it would remove all filters/neuron(s).\n",
            "Skipping pruning for fc2 as it would remove all filters/neuron(s).\n",
            "Skipping pruning for fc3 (output layer).\n",
            "Structural pruning based on zero ratio applied. 100 filters/neuron(s) pruned in total.\n",
            "Retraining after pruning\n",
            "Epoch [1/8], Loss: 0.0098, Accuracy: 87.87%, Val Loss: 0.0004, Val Accuracy: 100.00%\n",
            "Epoch [2/8], Loss: 0.0010, Accuracy: 98.82%, Val Loss: 0.0001, Val Accuracy: 99.96%\n",
            "Epoch [3/8], Loss: 0.0007, Accuracy: 99.15%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
            "Epoch [4/8], Loss: 0.0004, Accuracy: 99.53%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
            "Epoch [5/8], Loss: 0.0002, Accuracy: 99.74%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
            "Epoch [6/8], Loss: 0.0002, Accuracy: 99.73%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
            "Epoch [7/8], Loss: 0.0002, Accuracy: 99.82%, Val Loss: 0.0001, Val Accuracy: 99.85%\n",
            "Epoch [8/8], Loss: 0.0002, Accuracy: 99.80%, Val Loss: 0.0000, Val Accuracy: 100.00%\n",
            "Final evaluation on the test set...\n",
            "Final Test Loss: 0.0000, Final Test Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 테스트 평가\n",
        "print(\"Final evaluation on the test set...\")\n",
        "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "print(f'Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iegIjgL7Vfjd",
        "outputId": "f157b5a0-b073-4637-a2a4-a0906ec3a83a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final evaluation on the test set...\n",
            "Final Test Loss: 0.0000, Final Test Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 크기\n",
        "model_size = get_model_size(model)\n",
        "print(f\"Model Size: {model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqM46zUkVfg1",
        "outputId": "45e2c8b9-a68f-468f-9e8e-1709b884b93b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 276.09 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 비제로 가중치 계산 함수\n",
        "def count_nonzero_weights(model):\n",
        "    nonzero_count = 0\n",
        "    total_count = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            nonzero_count += torch.sum(param != 0).item()  # 0이 아닌 가중치 수 계산\n",
        "            total_count += param.numel()  # 전체 가중치 수 계산\n",
        "    return nonzero_count, total_count\n",
        "\n",
        "# 비제로 가중치 수 계산\n",
        "nonzero_weights, total_weights = count_nonzero_weights(model)\n",
        "print(f\"Number of non-zero weights: {nonzero_weights}\")\n",
        "print(f\"Total number of weights: {total_weights}\")\n",
        "print(f\"Percentage of non-zero weights: {100 * nonzero_weights / total_weights:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3OTiZQpXaL1",
        "outputId": "e1814f31-0ddb-4fe0-db7a-4c471616d698"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of non-zero weights: 71338817\n",
            "Total number of weights: 72374987\n",
            "Percentage of non-zero weights: 98.57%\n"
          ]
        }
      ]
    }
  ]
}