{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOYwNiWJuJTPfbEC589bssJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study3/blob/main/GaussianMagnitudeFusionPruning2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mc1bhQuncnT",
        "outputId": "333457e9-f134-4364-c5db-881c4ef49d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUfH4_dMnkyP",
        "outputId": "6250e986-f663-4d6e-e72a-a45eb54c922b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow_model_optimization\n",
            "Successfully installed tensorflow_model_optimization-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import norm\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "bp1RuWmwnk0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_motor10'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "def load_data(base_dir, split):\n",
        "    X = []\n",
        "    y = []\n",
        "    split_dir = os.path.join(base_dir, split)\n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(split_dir, category)\n",
        "        for file in os.listdir(category_dir):\n",
        "            file_path = os.path.join(category_dir, file)\n",
        "            data = pd.read_csv(file_path, header=None).values\n",
        "            data = pd.to_numeric(data.flatten(), errors='coerce').reshape(-1, data.shape[1])\n",
        "            data = np.nan_to_num(data).astype('float32')  # NaN 값을 0으로 대체하고, float32로 변환\n",
        "            X.append(data)\n",
        "            y.append(category)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 데이터 로드\n",
        "X_train, y_train = load_data(base_dir, 'train')\n",
        "X_val, y_val = load_data(base_dir, 'validation')\n",
        "X_test, y_test = load_data(base_dir, 'test')\n",
        "\n",
        "# 데이터 차원 변경 (CNN 입력 형식에 맞게)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# 원-핫 인코딩\n",
        "y_train_categorical = tf.keras.utils.to_categorical(y_train_encoded, num_classes=len(categories))\n",
        "y_val_categorical = tf.keras.utils.to_categorical(y_val_encoded, num_classes=len(categories))\n",
        "y_test_categorical = tf.keras.utils.to_categorical(y_test_encoded, num_classes=len(categories))"
      ],
      "metadata": {
        "id": "BJADvtt-nk2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "model = models.Sequential()\n",
        "\n",
        "# Conv1 레이어\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=16, strides=16, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "# Pool1 레이어\n",
        "model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
        "\n",
        "# Conv2 레이어\n",
        "model.add(layers.Conv1D(filters=32, kernel_size=3, strides=1, activation='relu'))\n",
        "\n",
        "# Conv3 레이어\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=5, strides=1, activation='relu'))\n",
        "\n",
        "# Conv4 레이어\n",
        "model.add(layers.Conv1D(filters=128, kernel_size=5, strides=1, activation='relu'))\n",
        "\n",
        "# Pool2 레이어\n",
        "model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
        "\n",
        "# Flatten 레이어\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# FC1 레이어\n",
        "model.add(layers.Dense(units=5000, activation='relu'))\n",
        "\n",
        "# FC2 레이어\n",
        "model.add(layers.Dense(units=1000, activation='relu'))\n",
        "\n",
        "# Output 레이어\n",
        "model.add(layers.Dense(len(categories), activation='softmax'))\n",
        "\n",
        "# 아담 옵티마이저 학습률 조정\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2R_zh2dnk5R",
        "outputId": "3d414f3a-075a-461a-9464-ffb0c1a8814f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 750, 64)           2112      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 375, 64)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 373, 32)           6176      \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 369, 64)           10304     \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 365, 128)          41088     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 182, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 23296)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5000)              116485000 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1000)              5001000   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 4004      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 121549684 (463.68 MB)\n",
            "Trainable params: 121549684 (463.68 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련\n",
        "history = model.fit(X_train, y_train_categorical, epochs=10, validation_data=(X_val, y_val_categorical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3B7KTaPnk75",
        "outputId": "21fa258c-1fda-4862-bbeb-3ce257eab55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "394/394 [==============================] - 203s 511ms/step - loss: 0.9640 - accuracy: 0.6066 - val_loss: 0.3151 - val_accuracy: 0.8333\n",
            "Epoch 2/10\n",
            "394/394 [==============================] - 200s 509ms/step - loss: 0.1152 - accuracy: 0.9583 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
            "Epoch 3/10\n",
            "394/394 [==============================] - 202s 512ms/step - loss: 0.1391 - accuracy: 0.9513 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "394/394 [==============================] - 202s 513ms/step - loss: 6.0000e-04 - accuracy: 1.0000 - val_loss: 6.0675e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "394/394 [==============================] - 201s 509ms/step - loss: 2.1900e-04 - accuracy: 1.0000 - val_loss: 7.2692e-04 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "394/394 [==============================] - 200s 508ms/step - loss: 8.7759e-05 - accuracy: 1.0000 - val_loss: 4.4510e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "394/394 [==============================] - 200s 507ms/step - loss: 0.1065 - accuracy: 0.9655 - val_loss: 0.0097 - val_accuracy: 0.9989\n",
            "Epoch 8/10\n",
            "394/394 [==============================] - 200s 508ms/step - loss: 0.0684 - accuracy: 0.9802 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "394/394 [==============================] - 200s 508ms/step - loss: 4.6700e-04 - accuracy: 1.0000 - val_loss: 3.1144e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "394/394 [==============================] - 201s 509ms/step - loss: 1.4532e-04 - accuracy: 1.0000 - val_loss: 1.0339e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi-FM-rQnk-a",
        "outputId": "0d7ebdd4-c363-436a-bd67-43df11fda246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - 3s 37ms/step - loss: 2.7821e-04 - accuracy: 1.0000\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원본 모델 저장\n",
        "model.save('model.h5')\n",
        "\n",
        "# 모델 사이즈 측정\n",
        "model_size = os.path.getsize('model.h5') / (1024 * 1024)\n",
        "print(f\"Model Size: {model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwrzWYDWnlBA",
        "outputId": "f5c5226e-306a-4a57-8f34-66a33148bed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 1391.09 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 비율 계산 함수\n",
        "def calculate_non_zero_weights(model):\n",
        "    total_weights = 0\n",
        "    non_zero_weights = 0\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel'):\n",
        "            weights = layer.get_weights()[0]\n",
        "            total_weights += weights.size\n",
        "            non_zero_weights += np.count_nonzero(weights)\n",
        "    return non_zero_weights, total_weights\n",
        "\n",
        "# 프루닝 전 가중치 비율 계산\n",
        "non_zero_weights_before, total_weights_before = calculate_non_zero_weights(model)\n",
        "print(f\"Before pruning: Non-zero weights = {non_zero_weights_before}, Total weights = {total_weights_before}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-BSAVfhnlDh",
        "outputId": "64c15358-6e83-47cc-f06a-75b9d340fdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before pruning: Non-zero weights = 121543385, Total weights = 121543392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1단계: 매그니튜드 기반 프루닝\n",
        "def magnitude_based_pruning(layer, sparsity):\n",
        "    \"\"\"매그니튜드 기반 프루닝\"\"\"\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        weights, biases = layer.get_weights()\n",
        "        threshold = np.percentile(np.abs(weights), sparsity * 100)\n",
        "        pruned_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
        "        layer.set_weights([pruned_weights, biases])\n",
        "\n",
        "# 2단계: 가우시안 기반 프루닝\n",
        "def gaussian_importance(weights):\n",
        "    \"\"\"가중치의 중요도를 가우시안 분포로 계산\"\"\"\n",
        "    mean = np.mean(weights)\n",
        "    std = np.std(weights)\n",
        "    importance = norm.pdf(weights, loc=mean, scale=std)\n",
        "    return importance\n",
        "\n",
        "def gaussian_pruning(layer, threshold_ratio=0.1429):  # 0.1429로 설정하여 추가로 10% 프루닝\n",
        "    \"\"\"가우시안 기반 프루닝\"\"\"\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        weights, biases = layer.get_weights()\n",
        "        importance = gaussian_importance(weights)\n",
        "        threshold = np.percentile(importance, threshold_ratio * 100)\n",
        "        pruned_weights = np.where(importance < threshold, 0, weights)\n",
        "        layer.set_weights([pruned_weights, biases])\n",
        "\n",
        "# 프루닝 수행: 먼저 매그니튜드 기반 프루닝 70%, 그 다음 가우시안 기반 프루닝 10% (실제로 14.29%)\n",
        "def combined_pruning(model, magnitude_sparsity=0.7, gaussian_threshold_ratio=0.1429):\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel'):\n",
        "            # 1단계: 매그니튜드 기반 프루닝 (70% 프루닝)\n",
        "            magnitude_based_pruning(layer, magnitude_sparsity)\n",
        "            # 2단계: 가우시안 기반 프루닝 (남은 30% 중에서 14.29% 추가 프루닝)\n",
        "            gaussian_pruning(layer, gaussian_threshold_ratio)\n",
        "\n",
        "# 예시: 모델의 각 레이어에 대해 프루닝 수행\n",
        "combined_pruning(model, magnitude_sparsity=0.7, gaussian_threshold_ratio=0.1429)"
      ],
      "metadata": {
        "id": "FOlhglB4nlGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 프루닝 후 가중치 비율 계산\n",
        "non_zero_weights_after, total_weights_after = calculate_non_zero_weights(model)\n",
        "print(f\"After pruning: Non-zero weights = {non_zero_weights_after}, Total weights = {total_weights_after}\")\n",
        "\n",
        "# 최종 가중치 비율 계산\n",
        "final_weight_ratio = non_zero_weights_after / total_weights_after\n",
        "print(f\"Final non-zero weight ratio: {final_weight_ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJdyIK2bnlIp",
        "outputId": "90a06706-fb2f-4ee2-ef53-5ff51e114a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After pruning: Non-zero weights = 19094481, Total weights = 121543392\n",
            "Final non-zero weight ratio: 0.1571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이후 모델 재학습 및 평가 수행\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train_categorical, epochs=10, validation_data=(X_val, y_val_categorical))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90umlMmxnlLK",
        "outputId": "25fac6a5-c2de-44fd-9609-22fbfed30d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "394/394 [==============================] - 203s 512ms/step - loss: 1.2499 - accuracy: 0.4966 - val_loss: 1.2429 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "394/394 [==============================] - 202s 514ms/step - loss: 0.6688 - accuracy: 0.7257 - val_loss: 0.2096 - val_accuracy: 0.9019\n",
            "Epoch 3/10\n",
            "394/394 [==============================] - 203s 514ms/step - loss: 0.1879 - accuracy: 0.9111 - val_loss: 0.1462 - val_accuracy: 0.9363\n",
            "Epoch 4/10\n",
            "394/394 [==============================] - 204s 518ms/step - loss: 0.1208 - accuracy: 0.9498 - val_loss: 0.0613 - val_accuracy: 0.9759\n",
            "Epoch 5/10\n",
            "394/394 [==============================] - 205s 519ms/step - loss: 0.1078 - accuracy: 0.9612 - val_loss: 0.1395 - val_accuracy: 0.9548\n",
            "Epoch 6/10\n",
            "394/394 [==============================] - 204s 519ms/step - loss: 0.0495 - accuracy: 0.9823 - val_loss: 0.0245 - val_accuracy: 0.9911\n",
            "Epoch 7/10\n",
            "394/394 [==============================] - 205s 519ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.1738 - val_accuracy: 0.9348\n",
            "Epoch 8/10\n",
            "394/394 [==============================] - 205s 521ms/step - loss: 0.0280 - accuracy: 0.9904 - val_loss: 0.0030 - val_accuracy: 0.9996\n",
            "Epoch 9/10\n",
            "394/394 [==============================] - 206s 522ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.0015 - val_accuracy: 0.9996\n",
            "Epoch 10/10\n",
            "394/394 [==============================] - 207s 526ms/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 0.0046 - val_accuracy: 0.9981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical)\n",
        "print(f'Test accuracy after combined pruning: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r2HIVmJnlNx",
        "outputId": "5e3c8938-2d40-403f-d50a-33947d4a531e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85/85 [==============================] - 4s 41ms/step - loss: 0.0142 - accuracy: 0.9952\n",
            "Test accuracy after combined pruning: 0.9951851963996887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스트립 프루닝 적용\n",
        "model_stripped = tfmot.sparsity.keras.strip_pruning(model)\n",
        "\n",
        "# 스트립 프루닝 후 모델 저장\n",
        "model_stripped.save('pruned_model_stripped.h5')\n",
        "\n",
        "# 스트립 프루닝 후 모델 사이즈 측정\n",
        "pruned_model_size = os.path.getsize('pruned_model_stripped.h5') / (1024 * 1024)\n",
        "print(f\"Stripped Pruned Model Size: {pruned_model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkVuDR8knlQZ",
        "outputId": "d276bd85-b19b-4fe6-e5c4-c46d33cdf344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stripped Pruned Model Size: 463.71 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfMDrZpCnlTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyvU92Qtnla5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}