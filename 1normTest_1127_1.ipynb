{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtXNWg/Pgh36VoQRRU3gJ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study3/blob/main/1normTest_1127_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucMNsYCkVyjC",
        "outputId": "88ec7b6a-6b0b-47e0-e1ee-f0cb310916e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_pruning\n",
        "!pip install ptflops\n",
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLqcEjEZV_iV",
        "outputId": "592c326c-1f4d-44da-f074-68883c578457"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_pruning\n",
            "  Downloading torch_pruning-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch_pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch_pruning) (3.0.2)\n",
            "Downloading torch_pruning-1.5.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_pruning\n",
            "Successfully installed torch_pruning-1.5.0\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.4\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=f2ba3a8329d4fad1e46ec60b138dcac6670bdbb0513d372db7b0095c43155306\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=a3fb0c10215adfaff3527736c1f7aa91f84db52d641390a5ec3487fbf2d2bbf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.0.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch_pruning as tp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from ptflops import get_model_complexity_info\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "hQEN6u8QV_k8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_motor30'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "class VibrationDataset(Dataset):\n",
        "    def __init__(self, base_dir, split, categories, label_encoder, segment_length=4000, transform=None):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.transform = transform\n",
        "        self.segment_length = segment_length\n",
        "        self.file_count = defaultdict(int)  # 카테고리별 파일 개수를 저장할 딕셔너리\n",
        "\n",
        "        split_dir = os.path.join(base_dir, split)\n",
        "        for category in categories:\n",
        "            category_dir = os.path.join(split_dir, category)\n",
        "            files = os.listdir(category_dir)\n",
        "            self.file_count[category] = len(files)  # 카테고리별 파일 수 저장\n",
        "\n",
        "            for file in files:\n",
        "                file_path = os.path.join(category_dir, file)\n",
        "                data = pd.read_csv(file_path, header=None, skiprows=1)  # 첫 행 건너뜀\n",
        "\n",
        "                # 첫 번째 열(시간 데이터)을 제외하고 주파수 데이터만 선택\n",
        "                frequency_data = data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "\n",
        "                # 데이터를 세그먼트로 분할\n",
        "                num_segments = frequency_data.shape[0] // self.segment_length\n",
        "                for i in range(num_segments):\n",
        "                    segment = frequency_data[i * self.segment_length:(i + 1) * self.segment_length]\n",
        "                    self.X.append(segment)  # 주파수 데이터만 포함\n",
        "                    self.y.append(label_encoder.transform([category])[0])\n",
        "\n",
        "        # 카테고리별 파일 개수 출력\n",
        "        print(f\"File count for '{split}' split:\")\n",
        "        for category, count in self.file_count.items():\n",
        "            print(f\"  {category}: {count} files\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx].T  # 2D 입력 (채널, 길이)로 변경\n",
        "        y = self.y[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(categories)\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_dataset = VibrationDataset(base_dir, 'train', categories, label_encoder)\n",
        "val_dataset = VibrationDataset(base_dir, 'validation', categories, label_encoder)\n",
        "test_dataset = VibrationDataset(base_dir, 'test', categories, label_encoder)\n",
        "\n",
        "# 데이터 로더\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 첫 번째 데이터의 크기를 이용해 input_length 결정\n",
        "first_sample, _ = train_dataset[0]\n",
        "input_length = first_sample.shape[1]  # X는 (채널, 길이) 형태이므로 길이는 첫 번째 데이터의 두 번째 차원\n",
        "print(f\"Input length for one sample: {input_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYabpOdlV_nj",
        "outputId": "022eb836-d41c-4078-82a6-7bd10070154a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File count for 'train' split:\n",
            "  normal: 2100 files\n",
            "  fault_BB: 2100 files\n",
            "  fault_RI: 2100 files\n",
            "  fault_SM: 2100 files\n",
            "File count for 'validation' split:\n",
            "  normal: 450 files\n",
            "  fault_BB: 450 files\n",
            "  fault_RI: 450 files\n",
            "  fault_SM: 450 files\n",
            "File count for 'test' split:\n",
            "  normal: 450 files\n",
            "  fault_BB: 450 files\n",
            "  fault_RI: 450 files\n",
            "  fault_SM: 450 files\n",
            "Input length for one sample: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 정의\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_length):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=16, stride=16)  # 입력 채널을 1로 수정\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=1)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # 출력 크기 계산\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.zeros(1, 1, input_length)  # 1은 입력 채널 수\n",
        "            sample_output = self.forward_conv_layers(sample_input)\n",
        "            conv_output_size = sample_output.size(1) * sample_output.size(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(conv_output_size, 5000)\n",
        "        #self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(5000, 1000)\n",
        "        self.fc3 = nn.Linear(1000, len(categories))\n",
        "\n",
        "    def forward_conv_layers(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.pool2(torch.relu(self.conv4(x)))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flattening\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        #x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FZTxGlB7V_qL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# 테스트 성능 평가\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "UUIHv0W6WAB9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNModel(input_length).to(device)\n",
        "# 손실 함수 및 옵티마이저\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
      ],
      "metadata": {
        "id": "CaDrYdolr96z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_and_apply_l1_norm_pruning_for_filters(model, prune_ratio=0.5, example_inputs=None):\n",
        "    if example_inputs is None:\n",
        "        example_inputs = torch.randn(1, 1, input_length).to(next(model.parameters()).device)  # 입력 예제\n",
        "\n",
        "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
        "    total_pruned = 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d):  # Conv1D 레이어만 대상으로 설정\n",
        "            # L1-노름 계산\n",
        "            weight_data = module.weight.detach().cpu().numpy()\n",
        "            l1_norm_scores = np.sum(np.abs(weight_data), axis=(1, 2))  # (out_channels, in_channels, kernel_size)\n",
        "\n",
        "            # 중요도가 낮은 필터 선택\n",
        "            num_filters = len(l1_norm_scores)\n",
        "            num_pruned = int(prune_ratio * num_filters)\n",
        "            prune_indices = np.argsort(l1_norm_scores)[:num_pruned]  # L1-노름 낮은 순으로 선택\n",
        "\n",
        "            # 프루닝 대상 필터 제거\n",
        "            if len(prune_indices) > 0 and len(prune_indices) < module.weight.shape[0]:\n",
        "                pruning_group = DG.get_pruning_group(module, tp.prune_conv_out_channels, idxs=prune_indices)\n",
        "\n",
        "                if pruning_group is not None:\n",
        "                    pruning_group.prune()\n",
        "                    total_pruned += len(prune_indices)\n",
        "                    print(f\"Pruned {len(prune_indices)} filters from {name}.\")\n",
        "            else:\n",
        "                print(f\"Skipping pruning for {name} as it would remove all filters.\")\n",
        "\n",
        "    print(f\"L1-norm-based filter pruning applied. {total_pruned} filters pruned in total.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "rOAI8Jm0sIBE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmpAqQeyWAEm",
        "outputId": "a80428ac-28b3-474b-e088-72d3b682f5a8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 1.3861976356675783\n",
            "Epoch 2/5, Loss: 1.2563602189878522\n",
            "Epoch 3/5, Loss: 0.6449919029086979\n",
            "Epoch 4/5, Loss: 0.3140986181587586\n",
            "Epoch 5/5, Loss: 0.13913490498901776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prune_ratio = 0.7  # 필터의 50% 제거\n",
        "model.eval()  # 프루닝은 eval 모드에서 수행\n",
        "model = detect_and_apply_l1_norm_pruning_for_filters(model, prune_ratio=prune_ratio)\n",
        "\n",
        "print(\"L1-norm-based filter pruning completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uMBBkc9r6jj",
        "outputId": "d9473373-7018-4ba6-bcc6-91a647f7c7d2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned 44 filters from conv1.\n",
            "Pruned 22 filters from conv2.\n",
            "Pruned 44 filters from conv3.\n",
            "Pruned 89 filters from conv4.\n",
            "L1-norm-based filter pruning applied. 199 filters pruned in total.\n",
            "L1-norm-based filter pruning completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dheAN-naBKH8",
        "outputId": "9a9b584f-af23-442d-cc68-bfd6c86d5ec2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.134242464896991\n",
            "Epoch 2/10, Loss: 1.019887001230027\n",
            "Epoch 3/10, Loss: 0.9525756379825815\n",
            "Epoch 4/10, Loss: 0.8924447757943632\n",
            "Epoch 5/10, Loss: 0.8420347490437745\n",
            "Epoch 6/10, Loss: 0.7995059481883412\n",
            "Epoch 7/10, Loss: 0.763145005264258\n",
            "Epoch 8/10, Loss: 0.7319373942646884\n",
            "Epoch 9/10, Loss: 0.7032504131648746\n",
            "Epoch 10/10, Loss: 0.6762537697169382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9uPf51sWAHg",
        "outputId": "b871c4ef-cdba-4598-ea9d-7b99851f3c30"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 구조 출력\n",
        "print(model)\n",
        "\n",
        "# 입력 데이터로 테스트\n",
        "test_input = torch.randn(1, 1, input_length).to(next(model.parameters()).device)\n",
        "output = model(test_input)\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-WqIKD6WAKW",
        "outputId": "ac7bedb4-e3ff-4b9a-bb65-cc8c17472447"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNModel(\n",
            "  (conv1): Conv1d(1, 20, kernel_size=(16,), stride=(16,))\n",
            "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv1d(20, 10, kernel_size=(3,), stride=(1,))\n",
            "  (conv3): Conv1d(10, 20, kernel_size=(5,), stride=(1,))\n",
            "  (conv4): Conv1d(20, 39, kernel_size=(5,), stride=(1,))\n",
            "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2223, out_features=5000, bias=True)\n",
            "  (fc2): Linear(in_features=5000, out_features=1000, bias=True)\n",
            "  (fc3): Linear(in_features=1000, out_features=4, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    model_size = (param_size + buffer_size) / 1024**2  # Convert to MB\n",
        "\n",
        "    if model_size < 1:\n",
        "        return model_size * 1024  # Convert to KB if size is less than 1MB\n",
        "    return model_size"
      ],
      "metadata": {
        "id": "miKR4wGfWAPs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = get_model_size(model)\n",
        "\n",
        "if model_size < 1:\n",
        "    print(f'Model size: {model_size * 1024:.2f} KB')\n",
        "else:\n",
        "    print(f'Model size: {model_size:.2f} MB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2vnQCWCWAMz",
        "outputId": "f7b106da-d16d-43d9-af3f-4b283bcdb215"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 61.53 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w8PiP0ByWASk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-zN7sgNWAVU"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXmq81s8WAX8"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nh4I6TPDWAa0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c46YF6cqWAdq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miZFaBtdWAgD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7dWAWADUWAjY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLbweCSdWAx8"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGOiEBIEWA0s"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EgPfQJ9WA3k"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4N8xnC_WA6M"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}