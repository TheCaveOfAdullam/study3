{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgx4rKHWCa57FZyz5FBSBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study3/blob/main/chPruningTest_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCr-u6vy5w6y",
        "outputId": "3544b38e-5a7e-476a-b799-5e261887fcfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_pruning\n",
        "!pip install ptflops\n",
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTfEaL_c6CN8",
        "outputId": "990b1b80-c864-4d56-ed34-b1bdf9cb7ef9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_pruning\n",
            "  Downloading torch_pruning-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch_pruning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch_pruning) (3.0.2)\n",
            "Downloading torch_pruning-1.5.0-py3-none-any.whl (63 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_pruning\n",
            "Successfully installed torch_pruning-1.5.0\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from ptflops) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.4\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=d19a2147a67efe7c7e16f470158988085c9f1392c0272eff904fc6d5a096cac0\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=2425916ae98b700c43ac3eaa15e493d6bc967f13e4f10d34e35ee7d630832d2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.0.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch_pruning as tp"
      ],
      "metadata": {
        "id": "HLu2WijF6CTj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_motor30'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "class VibrationDataset(Dataset):\n",
        "    def __init__(self, base_dir, split, categories, label_encoder, segment_length=4000, transform=None):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.transform = transform\n",
        "        self.segment_length = segment_length\n",
        "        self.file_count = defaultdict(int)\n",
        "\n",
        "        split_dir = os.path.join(base_dir, split)\n",
        "        for category in categories:\n",
        "            category_dir = os.path.join(split_dir, category)\n",
        "            files = os.listdir(category_dir)\n",
        "            self.file_count[category] = len(files)\n",
        "\n",
        "            for file in files:\n",
        "                file_path = os.path.join(category_dir, file)\n",
        "                data = pd.read_csv(file_path, header=None, skiprows=1)\n",
        "                frequency_data = data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "                num_segments = frequency_data.shape[0] // self.segment_length\n",
        "                for i in range(num_segments):\n",
        "                    segment = frequency_data[i * self.segment_length:(i + 1) * self.segment_length]\n",
        "                    self.X.append(segment)\n",
        "                    self.y.append(label_encoder.transform([category])[0])\n",
        "\n",
        "        print(f\"File count for '{split}' split:\")\n",
        "        for category, count in self.file_count.items():\n",
        "            print(f\"  {category}: {count} files\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx].T\n",
        "        y = self.y[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(categories)\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_dataset = VibrationDataset(base_dir, 'train', categories, label_encoder)\n",
        "val_dataset = VibrationDataset(base_dir, 'validation', categories, label_encoder)\n",
        "test_dataset = VibrationDataset(base_dir, 'test', categories, label_encoder)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "first_sample, _ = train_dataset[0]\n",
        "input_length = first_sample.shape[1]\n",
        "print(f\"Input length for one sample: {input_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jooD9vWm6CWK",
        "outputId": "4cfd1d29-8d81-4a5c-c6e8-0a3fad94ced8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File count for 'train' split:\n",
            "  normal: 2100 files\n",
            "  fault_BB: 2100 files\n",
            "  fault_RI: 2100 files\n",
            "  fault_SM: 2100 files\n",
            "File count for 'validation' split:\n",
            "  normal: 450 files\n",
            "  fault_BB: 450 files\n",
            "  fault_RI: 450 files\n",
            "  fault_SM: 450 files\n",
            "File count for 'test' split:\n",
            "  normal: 450 files\n",
            "  fault_BB: 450 files\n",
            "  fault_RI: 450 files\n",
            "  fault_SM: 450 files\n",
            "Input length for one sample: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 정의\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_length):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=16, stride=16)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=1)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.zeros(1, 1, input_length)\n",
        "            sample_output = self.forward_conv_layers(sample_input)\n",
        "            conv_output_size = sample_output.size(1) * sample_output.size(2)\n",
        "\n",
        "        self.fc1 = nn.Linear(conv_output_size, 5000)\n",
        "        self.fc2 = nn.Linear(5000, 1000)\n",
        "        self.fc3 = nn.Linear(1000, len(categories))\n",
        "\n",
        "    def forward_conv_layers(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.pool2(torch.relu(self.conv4(x)))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel(input_length).to(device)"
      ],
      "metadata": {
        "id": "Uu5dViTo6CYj"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_gradients(model, dataloader, device, criterion):\n",
        "    mean_gradients = {}\n",
        "    model.train()  # 기울기 계산을 위해 train 모드로 설정\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d):\n",
        "            # 기울기를 저장할 리스트 초기화\n",
        "            channel_gradients = []\n",
        "\n",
        "            def hook_fn(grad):\n",
        "                # 채널별로 기울기의 절댓값 평균을 계산하여 저장\n",
        "                grad = grad.detach().cpu().numpy()\n",
        "                channel_mean_grad = np.mean(np.abs(grad), axis=(1, 2))\n",
        "                channel_gradients.append(channel_mean_grad)\n",
        "\n",
        "            # 가중치에 대한 hook 등록\n",
        "            hook = module.weight.register_hook(hook_fn)\n",
        "\n",
        "            # 데이터셋에 대한 순전파 및 역전파 수행\n",
        "            for inputs, targets in dataloader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                model.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "\n",
        "            # hook 제거\n",
        "            hook.remove()\n",
        "\n",
        "            # 채널별 평균 기울기 계산\n",
        "            mean_gradient = np.mean(channel_gradients, axis=0)\n",
        "            mean_gradients[name] = mean_gradient\n",
        "\n",
        "    return mean_gradients"
      ],
      "metadata": {
        "id": "xmadqZYy6CbC"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_model_based_on_mean_gradient(model, mean_gradients, prune_ratio=0.2, example_inputs=None):\n",
        "    device = next(model.parameters()).device\n",
        "    if example_inputs is None:\n",
        "        example_inputs = torch.randn(1, 1, input_length).to(device)  # 입력 채널을 1로 설정\n",
        "\n",
        "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
        "    total_pruned = 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d):\n",
        "            # 출력 레이어는 프루닝 대상에서 제외 (필요에 따라 수정)\n",
        "            if name == 'fc3':\n",
        "                print(f\"Skipping pruning for {name} (output layer).\")\n",
        "                continue\n",
        "\n",
        "            # 채널별 Mean Gradient 가져오기\n",
        "            if name in mean_gradients:\n",
        "                channel_mean_grad = mean_gradients[name]\n",
        "            else:\n",
        "                continue  # 해당 레이어의 기울기를 계산하지 않았다면 스킵\n",
        "\n",
        "            # 프루닝할 채널 수 계산\n",
        "            num_channels = len(channel_mean_grad)\n",
        "            num_prune = int(num_channels * prune_ratio)\n",
        "            if num_prune < 1:\n",
        "                continue  # 프루닝할 채널이 없으면 스킵\n",
        "\n",
        "            # 중요도가 낮은 채널의 인덱스 선택\n",
        "            prune_indices = np.argsort(channel_mean_grad)[:num_prune]\n",
        "\n",
        "            # 프루닝 대상 채널 제거\n",
        "            if len(prune_indices) > 0 and len(prune_indices) < num_channels:\n",
        "                pruning_group = DG.get_pruning_group(module, tp.prune_conv_out_channels, idxs=prune_indices)\n",
        "\n",
        "                if DG.check_pruning_group(pruning_group):\n",
        "                    pruning_group.prune()\n",
        "                    total_pruned += len(prune_indices)\n",
        "                    print(f\"Pruned {len(prune_indices)} channels from {name}.\")\n",
        "                else:\n",
        "                    print(f\"Cannot prune {name} due to dependency constraints.\")\n",
        "            else:\n",
        "                print(f\"Skipping pruning for {name} as it would remove all channels.\")\n",
        "\n",
        "    print(f\"Mean Gradient-based pruning applied. {total_pruned} channels pruned in total.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "MiuXTVQmD4wG"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 학습 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
      ],
      "metadata": {
        "id": "4a1hw_Bd6Cdj"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5  # 원하는 에포크 수로 설정\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "Rz6vpyoPGyVu"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Mean Gradient 계산\n",
        "mean_gradients = calculate_mean_gradients(model, train_loader, device, criterion)"
      ],
      "metadata": {
        "id": "2_Hfit9J6CgC"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 입력 예제 생성 (입력 채널 수와 길이에 맞게 수정)\n",
        "input_channels = 1  # 모델의 입력 채널 수에 맞게 설정\n",
        "input_length = 4000  # 입력 데이터의 길이에 맞게 설정\n",
        "example_inputs = torch.randn(1, input_channels, input_length).to(device)"
      ],
      "metadata": {
        "id": "Lx59KKey6Cii"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 모델 프루닝\n",
        "prune_ratio = 0.90  # 프루닝할 비율 (예: 20%)\n",
        "model = prune_model_based_on_mean_gradient(model, mean_gradients, prune_ratio, example_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zhS-_u46Ck6",
        "outputId": "6f09b93a-d4e8-446b-afef-8e2c6ddb2523"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned 57 channels from conv1.\n",
            "Pruned 28 channels from conv2.\n",
            "Pruned 57 channels from conv3.\n",
            "Pruned 115 channels from conv4.\n",
            "Mean Gradient-based pruning applied. 257 channels pruned in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 예제 생성 (입력 채널 수와 길이에 맞게 수정)\n",
        "input_channels = 1  # 입력 채널 수\n",
        "input_length = 4000  # 입력 데이터의 길이\n",
        "example_inputs = torch.randn(1, input_channels, input_length)"
      ],
      "metadata": {
        "id": "CuEoT5TyBhHF"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 프루닝 후 모델을 디바이스로 이동\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQNLt4A16Cni",
        "outputId": "dbf364d0-0b29-4846-a4ef-a8c03265354a"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (conv1): Conv1d(1, 7, kernel_size=(16,), stride=(16,))\n",
              "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv1d(7, 4, kernel_size=(3,), stride=(1,))\n",
              "  (conv3): Conv1d(4, 7, kernel_size=(5,), stride=(1,))\n",
              "  (conv4): Conv1d(7, 13, kernel_size=(5,), stride=(1,))\n",
              "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=741, out_features=5000, bias=True)\n",
              "  (fc2): Linear(in_features=5000, out_features=1000, bias=True)\n",
              "  (fc3): Linear(in_features=1000, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 옵티마이저 재설정\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
      ],
      "metadata": {
        "id": "ywgH9sRkBmnd"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. 파인튜닝 실행\n",
        "epochs = 10  # 원하는 에포크 수로 설정\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "WsuctbM76CqM"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 평가\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wea_2Fn6CvU",
        "outputId": "f636357f-ba19-4a41-e1fa-b0b33cd68f57"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 크기 계산\n",
        "def get_model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    model_size = (param_size + buffer_size) / 1024**2  # Convert to MB\n",
        "    return model_size"
      ],
      "metadata": {
        "id": "cGm5LWGq6Cxy"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 크기 출력\n",
        "model_size = get_model_size(model)\n",
        "print(f'Model size: {model_size:.2f} MB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVV4pDv06C0c",
        "outputId": "1cee7ff0-6968-4681-bde6-bbef436447f7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 33.25 MB\n"
          ]
        }
      ]
    }
  ]
}