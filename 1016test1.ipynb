{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM4slFZ3Yeg1rb3vS5Gmw2V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheCaveOfAdullam/study3/blob/main/1016test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIurm-kv-9oI",
        "outputId": "17bed2b9-10f5-4269-e0ad-1bd281bef5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_pruning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtRjEjI6_JD-",
        "outputId": "cc183d0c-5c8c-490d-fb5b-dda31f7d621a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_pruning\n",
            "  Downloading torch_pruning-1.4.3-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (2.4.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch_pruning) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torch_pruning) (1.3.0)\n",
            "Downloading torch_pruning-1.4.3-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_pruning\n",
            "Successfully installed torch_pruning-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch_pruning as tp"
      ],
      "metadata": {
        "id": "-7rtGJMo_JGt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/ship_motor10'\n",
        "categories = ['normal', 'fault_BB', 'fault_RI', 'fault_SM']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 정의\n",
        "class VibrationDataset(Dataset):\n",
        "    def __init__(self, base_dir, split, categories, transform=None):\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.transform = transform\n",
        "        split_dir = os.path.join(base_dir, split)\n",
        "        for category in categories:\n",
        "            category_dir = os.path.join(split_dir, category)\n",
        "            files = os.listdir(category_dir)\n",
        "            for file in files:\n",
        "                file_path = os.path.join(category_dir, file)\n",
        "                data = pd.read_csv(file_path, header=None).values\n",
        "                data = pd.to_numeric(data.flatten(), errors='coerce').reshape(-1, data.shape[1])\n",
        "                data = np.nan_to_num(data).astype('float32')\n",
        "                self.X.append(data)\n",
        "                self.y.append(category)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        X = X.reshape(-1)\n",
        "        return torch.tensor(X, dtype=torch.float32), y\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_dataset = VibrationDataset(base_dir, 'train', categories)\n",
        "val_dataset = VibrationDataset(base_dir, 'validation', categories)\n",
        "test_dataset = VibrationDataset(base_dir, 'test', categories)\n",
        "\n",
        "# 레이블 인코딩\n",
        "y_train_encoded = label_encoder.fit_transform([y for _, y in train_dataset])\n",
        "y_val_encoded = label_encoder.transform([y for _, y in val_dataset])\n",
        "y_test_encoded = label_encoder.transform([y for _, y in test_dataset])\n",
        "\n",
        "# 레이블 인코딩된 데이터셋 업데이트\n",
        "train_dataset.y = y_train_encoded\n",
        "val_dataset.y = y_val_encoded\n",
        "test_dataset.y = y_test_encoded\n",
        "\n",
        "# 데이터 로더\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "sf_4gYJE_JJI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 정의\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=16, stride=16)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1)\n",
        "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, stride=1)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        conv1_output_size = (24002 - 16) // 16 + 1\n",
        "        pool1_output_size = conv1_output_size // 2\n",
        "        conv2_output_size = (pool1_output_size - 3) // 1 + 1\n",
        "        conv3_output_size = (conv2_output_size - 5) // 1 + 1\n",
        "        conv4_output_size = (conv3_output_size - 5) // 1 + 1\n",
        "        pool2_output_size = conv4_output_size // 2\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * pool2_output_size, 5000)\n",
        "        self.fc2 = nn.Linear(5000, 1000)\n",
        "        self.fc3 = nn.Linear(1000, len(categories))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 손실 함수 및 옵티마이저 설정\n",
        "model = CNNModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "zebNmTrA_JLX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련 함수\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.unsqueeze(1).to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/total:.4f}, Accuracy: {100 * correct/total:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "# 모델 평가 함수\n",
        "def evaluate_model(model, loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.unsqueeze(1).to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return val_loss / total, 100 * correct / total"
      ],
      "metadata": {
        "id": "IQafEHen_JNk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.utils.prune as prune\n",
        "# import numpy as np\n",
        "# import torchprune as tp\n",
        "\n",
        "# # 비구조적 프루닝 적용\n",
        "# def apply_unstructured_pruning(model, amount=0.5):\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "#             prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "#             prune.remove(module, 'weight')  # 비구조적 프루닝 후 마스크 제거\n",
        "#     print(f\"Unstructured pruning applied with {amount * 100}% of weights pruned.\")\n",
        "#     return model\n",
        "\n",
        "# # 0 가중치 필터 감지 (최소 한 개의 필터는 남기도록 조정)\n",
        "# def detect_filters_to_prune(model, threshold=0.7):\n",
        "#     filters_to_prune = []\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, nn.Conv1d):\n",
        "#             weight_data = module.weight.detach().cpu().numpy()\n",
        "#             filter_zero_percentage = np.mean(weight_data == 0, axis=(1, 2))  # 각 필터의 0 비율 계산\n",
        "#             prune_indices = np.where(filter_zero_percentage >= threshold)[0]  # 70% 이상인 필터의 인덱스\n",
        "\n",
        "#             # 최소 한 개의 필터는 남기기 위한 조건 추가\n",
        "#             if len(prune_indices) > 0 and len(prune_indices) < module.weight.shape[0]:\n",
        "#                 filters_to_prune.append((module, prune_indices))\n",
        "#             else:\n",
        "#                 print(f\"Skipping pruning for {name} as it would remove all filters.\")\n",
        "#     return filters_to_prune\n",
        "\n",
        "# # 모델의 예시 입력 정의 (필요한 입력의 모양 제공)\n",
        "# example_inputs = torch.randn(1, 1, 24002).to(device)  # (배치 크기, 채널 수, 시퀀스 길이)\n",
        "\n",
        "# # 구조적 프루닝 적용 및 채널 불일치 해결\n",
        "# def apply_structural_pruning_with_torchprune(model, filters_to_prune):\n",
        "#     # DependencyGraph에서 종속성 그래프를 생성하고 예시 입력을 통해 분석\n",
        "#     DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
        "\n",
        "#     for module, prune_indices in filters_to_prune:\n",
        "#         # get_pruning_group을 사용하여 프루닝 그룹을 가져옵니다.\n",
        "#         pruning_group = DG.get_pruning_group(module, tp.prune_conv_out_channels, idxs=prune_indices)\n",
        "\n",
        "#         # 프루닝 그룹을 실행하여 프루닝 적용\n",
        "#         if pruning_group is not None:\n",
        "#             pruning_group.prune()\n",
        "#             print(f\"Pruned {len(prune_indices)} filters from module {module}.\")\n",
        "#         else:\n",
        "#             print(f\"Skipping module {module} as no pruning group was generated.\")\n",
        "\n",
        "#     print(f\"Structural pruning applied. {len(filters_to_prune)} filters pruned.\")\n",
        "#     return model\n",
        "\n",
        "# # 채널 불일치를 해결하기 위한 1x1 컨볼루션 레이어 추가\n",
        "# def add_1x1_conv_if_needed(model):\n",
        "#     new_layers = []\n",
        "#     for name, module in model.named_children():\n",
        "#         new_layers.append(module)\n",
        "#         if isinstance(module, nn.Conv1d):\n",
        "#             out_channels = module.out_channels\n",
        "#             in_channels = module.in_channels\n",
        "#             # 출력 채널이 입력 채널과 다를 때 1x1 컨볼루션 추가\n",
        "#             if out_channels != in_channels:\n",
        "#                 new_layers.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n",
        "#                 print(f\"Added 1x1 Conv1d to match channels after layer {name}.\")\n",
        "#     return nn.Sequential(*new_layers)\n",
        "\n",
        "# # 모델 프루닝 통합 함수\n",
        "# def prune_model(model, unstructured_amount=0.5, zero_threshold=0.7):\n",
        "#     model = apply_unstructured_pruning(model, amount=unstructured_amount)\n",
        "#     filters_to_prune = detect_filters_to_prune(model, threshold=zero_threshold)\n",
        "#     if filters_to_prune:\n",
        "#         model = apply_structural_pruning_with_torchprune(model, filters_to_prune)\n",
        "#         model = add_1x1_conv_if_needed(model)  # 프루닝 후 채널 불일치 해결\n",
        "#     else:\n",
        "#         print(\"No filters meet the zero weight threshold for pruning.\")\n",
        "#     return model\n",
        "\n",
        "# # 모델 훈련 함수\n",
        "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         for inputs, labels in train_loader:\n",
        "#             inputs, labels = inputs.unsqueeze(1).to(device), labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#         # 검증 로직을 여기에 추가할 수 있습니다.\n",
        "#         print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
        "#     return model"
      ],
      "metadata": {
        "id": "S4Vnvf9R_JQa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import numpy as np\n",
        "import torch_pruning as tp  # torch_pruning 라이브러리 사용\n",
        "\n",
        "# 비구조적 프루닝 적용\n",
        "def apply_unstructured_pruning(model, amount=0.5):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            prune.remove(module, 'weight')  # 비구조적 프루닝 후 마스크 제거\n",
        "    print(f\"Unstructured pruning applied with {amount * 100}% of weights pruned.\")\n",
        "    return model\n",
        "\n",
        "# 0 가중치 필터 감지 (최소 한 개의 필터는 남기도록 조정)\n",
        "def detect_filters_to_prune(model, threshold=0.7):\n",
        "    filters_to_prune = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv1d):\n",
        "            weight_data = module.weight.detach().cpu().numpy()\n",
        "            filter_zero_percentage = np.mean(weight_data == 0, axis=(1, 2))  # 각 필터의 0 비율 계산\n",
        "            prune_indices = np.where(filter_zero_percentage >= threshold)[0]  # 70% 이상인 필터의 인덱스\n",
        "\n",
        "            # 최소 한 개의 필터는 남기기 위한 조건 추가\n",
        "            if len(prune_indices) > 0 and len(prune_indices) < module.weight.shape[0]:\n",
        "                filters_to_prune.append((module, prune_indices))\n",
        "            else:\n",
        "                print(f\"Skipping pruning for {name} as it would remove all filters.\")\n",
        "    return filters_to_prune\n",
        "\n",
        "# 모델의 예시 입력 정의 (필요한 입력의 모양 제공)\n",
        "example_inputs = torch.randn(1, 1, 24002).to(device)  # (배치 크기, 채널 수, 시퀀스 길이)\n",
        "\n",
        "# 구조적 프루닝 적용 및 채널 불일치 해결 (torch_pruning 활용)\n",
        "def apply_structural_pruning_with_torchprune(model, filters_to_prune):\n",
        "    # DependencyGraph에서 종속성 그래프를 생성하고 예시 입력을 통해 분석\n",
        "    DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
        "\n",
        "    for module, prune_indices in filters_to_prune:\n",
        "        # get_pruning_group을 사용하여 프루닝 그룹을 가져옵니다.\n",
        "        pruning_group = DG.get_pruning_group(module, tp.prune_conv_out_channels, idxs=prune_indices)\n",
        "\n",
        "        # 프루닝 그룹을 실행하여 프루닝 적용 (pruning_group 객체의 prune 메서드 사용)\n",
        "        if pruning_group is not None:\n",
        "            pruning_group.prune()  # 프루닝 그룹의 prune 메서드 호출\n",
        "            print(f\"Pruned {len(prune_indices)} filters from module {module}.\")\n",
        "        else:\n",
        "            print(f\"Skipping module {module} as no pruning group was generated.\")\n",
        "\n",
        "    print(f\"Structural pruning applied. {len(filters_to_prune)} filters pruned.\")\n",
        "    return model\n",
        "\n",
        "# 모델 프루닝 통합 함수\n",
        "def prune_model(model, unstructured_amount=0.5, zero_threshold=0.7):\n",
        "    model = apply_unstructured_pruning(model, amount=unstructured_amount)\n",
        "    filters_to_prune = detect_filters_to_prune(model, threshold=zero_threshold)\n",
        "    if filters_to_prune:\n",
        "        model = apply_structural_pruning_with_torchprune(model, filters_to_prune)\n",
        "    else:\n",
        "        print(\"No filters meet the zero weight threshold for pruning.\")\n",
        "    return model\n",
        "\n",
        "# 모델 훈련 함수\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.unsqueeze(1).to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 검증 로직을 여기에 추가할 수 있습니다.\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "tZ7e5Oym_JSa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련 및 프루닝 테스트\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W882hEmP_JVE",
        "outputId": "b4a4a521-b10b-4c03-8e24-4d576b237a4b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7 completed.\n",
            "Epoch 2/7 completed.\n",
            "Epoch 3/7 completed.\n",
            "Epoch 4/7 completed.\n",
            "Epoch 5/7 completed.\n",
            "Epoch 6/7 completed.\n",
            "Epoch 7/7 completed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (conv1): Conv1d(1, 64, kernel_size=(16,), stride=(16,))\n",
              "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
              "  (conv3): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
              "  (conv4): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
              "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=47360, out_features=5000, bias=True)\n",
              "  (fc2): Linear(in_features=5000, out_features=1000, bias=True)\n",
              "  (fc3): Linear(in_features=1000, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model = prune_model(model, unstructured_amount=0.8, zero_threshold=0.70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddi5lUuxABWW",
        "outputId": "ae616881-d4f3-4977-c7e8-b62bb508c48b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unstructured pruning applied with 80.0% of weights pruned.\n",
            "Skipping pruning for conv4 as it would remove all filters.\n",
            "Pruned 48 filters from module Conv1d(1, 16, kernel_size=(16,), stride=(16,)).\n",
            "Pruned 30 filters from module Conv1d(16, 2, kernel_size=(3,), stride=(1,)).\n",
            "Pruned 61 filters from module Conv1d(2, 3, kernel_size=(5,), stride=(1,)).\n",
            "Structural pruning applied. 3 filters pruned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-4G0DZ4ABY1",
        "outputId": "ce529529-9ddb-4125-9433-f14be1918386"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 completed.\n",
            "Epoch 2/10 completed.\n",
            "Epoch 3/10 completed.\n",
            "Epoch 4/10 completed.\n",
            "Epoch 5/10 completed.\n",
            "Epoch 6/10 completed.\n",
            "Epoch 7/10 completed.\n",
            "Epoch 8/10 completed.\n",
            "Epoch 9/10 completed.\n",
            "Epoch 10/10 completed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (conv1): Conv1d(1, 16, kernel_size=(16,), stride=(16,))\n",
              "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv1d(16, 2, kernel_size=(3,), stride=(1,))\n",
              "  (conv3): Conv1d(2, 3, kernel_size=(5,), stride=(1,))\n",
              "  (conv4): Conv1d(3, 128, kernel_size=(5,), stride=(1,))\n",
              "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=47360, out_features=5000, bias=True)\n",
              "  (fc2): Linear(in_features=5000, out_features=1000, bias=True)\n",
              "  (fc3): Linear(in_features=1000, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 성능 확인\n",
        "test_loss, test_accuracy = evaluate_model(pruned_model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giFlJwH8ABbN",
        "outputId": "2410b596-bae2-41a3-fc7d-e41e933af2c6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0201, Test Accuracy: 83.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 비제로 가중치 계산 함수\n",
        "def count_nonzero_weights(model):\n",
        "    nonzero_count = 0\n",
        "    total_count = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            nonzero_count += torch.sum(param != 0).item()  # 0이 아닌 가중치 수 계산\n",
        "            total_count += param.numel()  # 전체 가중치 수 계산\n",
        "    return nonzero_count, total_count\n",
        "\n",
        "# 비제로 가중치 수 계산\n",
        "nonzero_weights, total_weights = count_nonzero_weights(model)\n",
        "print(f\"Number of non-zero weights: {nonzero_weights}\")\n",
        "print(f\"Total number of weights: {total_weights}\")\n",
        "print(f\"Percentage of non-zero weights: {100 * nonzero_weights / total_weights:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waA6DnLVABdx",
        "outputId": "6c510ffa-e4a8-48d2-c436-86117b2dfe92"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of non-zero weights: 58937307\n",
            "Total number of weights: 241812455\n",
            "Percentage of non-zero weights: 24.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ie20vA2DABgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCe6cI2pABh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}